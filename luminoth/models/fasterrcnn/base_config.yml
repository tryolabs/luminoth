train:
  # Run on debug mode (which enables more logging)
  debug: False
  # Training batch size for images. FasterRCNN currently only supports 1
  batch_size: 1
  # Where to save the models
  model_dir: models/
  # Checkpoint file from where to load existing var values
  checkpoint_file:
  # Ignore scope when loading from checkpoint (useful when training RPN first
  # and then RPN + RCNN)
  ignore_scope:
  # Where to save the logs for TensorBoard
  log_dir: /tmp/luminoth/
  # Save a checkpoint after that many batches
  save_every: 1000
  # Enables TensorFlow debug mode, which stops and lets you analyze Tensors
  # after each Session.run
  tf_debug: False
  # Run name is used for better checkpoint and log hierarchy
  run_name:
  # Disables logging and saving checkpoints
  no_log: False
  # Displays debugging images with results every N batches. Debug mode must be
  # enabled
  display_every: 1
  # Shuffle the dataset. It should only be disabled when trying to reproduce
  # some problem on some sample
  random_shuffle: True
  # Save Tensorboard timeline
  save_timeline: False
  # Save a training summary for TensorBoard every that many batches
  summary_every: 1
  # Run TensorFlow using full_trace mode for memory and running time logging
  # Debug mode must be enabled.
  full_trace: False
  # Initial learning rate, independent of the optimizer being used
  initial_learning_rate: 0.0001
  # When using learning rate decay, use this to decay after that many batches
  learning_rate_decay: 400000
  # Learning rate decay method ((empty), "none", piecewise_constant, exponential_decay)
  learning_rate_decay_method: piecewise_constant
  # Type of optimizer to use (momentum, adam)
  optimizer_type: momentum
  # When using momentum optimizer which value to use for momentum
  momentum: 0.9
  # Number of epochs (complete dataset batches) to run
  num_epochs: 10000

  # TODO: Remove this arg passed by Google Cloud ML
  job_dir: null

dataset:
  # From which directory to read the dataset
  dir: 'datasets/voc/tf'
  # Which split of tfrecords to look for
  split: train
  # Resize image according to min_size and max_size
  image_preprocessing:
    min_size: 600
    max_size: 1024
  # Data augmentation techniques
  data_augmentation:
    - flip:
        left_right: True
        up_down: False
        prob: 0.5
    - patch:
        min_height: 400
        min_width: 400
        prob: 0.2
    - resize:
        min_size: 400
        max_size: 980
        prob: 1.
    - distortion:
        brightness:
          enable: True
          max_delta: 0.3
        contrast:
          enable: True
          lower: 0.4
          upper: 0.8
        hue:
          enable: True
          max_delta: 0.2
        prob: 1.

network:
  # Total number of classes to predict
  num_classes: 20
  # Use RCNN or just RPN
  with_rcnn: True

pretrained:
  # Which type of pretrained network to use
  net: vgg_16
  # Should we train the pretrained network
  trainable: True
  # From which file to load the weights
  weights:
  # Which endpoint layer to use as feature map for network
  endpoint: vgg_16/conv5/conv5_1
  # Is trainable, how many layers from the endpoint are we training
  finetune_num_layers: 3
  # Regularization
  weight_decay: 0.0005

loss:
  # Loss weights for calculating the total loss
  rpn_cls_loss_weight: 1.0
  rpn_reg_loss_weights: 2.0
  rcnn_cls_loss_weight: 1.0
  rcnn_reg_loss_weights: 2.0

anchors:
  # Base size to use for anchors
  base_size: 256
  # Scale used for generating anchor sizes
  scales: [0.5, 1, 2]
  # Aspect ratios used for generating anchors
  ratios: [0.5, 1, 2]
  # Stride depending on feature map size (of pretrained)
  stride: 16

rpn:
  num_channels: 512
  kernel_shape: [3, 3]
  # Initializer to be used for all RPN conv layers (TODO: Different ones for cls & reg?)
  initializer:
    type: truncated_normal_initializer
    mean: 0.0
    stddev: 0.01
  l2_regularization_scale: 0.0005  # disable using 0
  activation_function: relu6

  proposals:
    # Total proposals to use before running NMS (sorted by score)
    pre_nms_top_n: 12000
    # Total proposals to use after NMS (sorted by score)
    post_nms_top_n: 2000
    # NMS threshold used when removing "almost duplicates"
    nms_threshold: 0.6
    min_size: 0  # disable using 0

  target:
    # Margin to crop propocals to close to the border
    allowed_border: 0
    # Overwrite positives with negative if threshold is too low
    clobber_positives: False
    # How much IoU with GT proposals must have to be marked as positive
    foreground_threshold: 0.7
    # High and low thresholds with GT to be considered background
    background_threshold_high: 0.3
    background_threshold_low: 0.
    foreground_fraction: 0.5
    # Ration between background and foreground in minibatch
    minibatch_size: 256
    # Assign to get consistent "random" selection in batch
    random_seed:  # only to be used for debugging

rcnn:
  enabled: True  # if disabled, then the rest of the config values will not be used.
  # FC layer sizes
  layer_sizes: [4096, 4096]
  # Dropout keep probability (turn off with 1.0 value)
  dropout_keep_prop: 1.0
  activation_function: relu6
  initializer:
    type: variance_scaling_initializer
    factor: 1.0
    uniform: True
    mode: FAN_AVG
  l2_regularization_scale: 0.0005

  roi:
    pooling_mode: crop
    pooled_width: 7
    pooled_height: 7
    padding: VALID

  proposals:
    # Maximum number of detections for each class
    class_max_detections: 100
    # NMS threshold used to remove "almost duplicate" of the same class
    class_nms_threshold: 0.6
    # Maximum total detections for an image (sorted by score)
    total_max_detections: 300

  target:
    # Ratio between foreground and background samples in minibatch
    foreground_fraction: 0.25
    minibatch_size: 64
    # Threshold with GT to be considered positive
    foreground_threshold: 0.5
    # High and low threshold with GT to be considered negative
    background_threshold_high: 0.5
    background_threshold_low: 0.1
