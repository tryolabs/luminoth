train:
  # Run on debug mode (which enables more logging)
  debug: True
  # Seed for random operations
  seed:
  # Training batch size for images.
  batch_size: 1
  # Directory in which model checkpoints & summaries (for Tensorboard) will be saved
  job_dir: jobs/
  # Enables TensorFlow debug mode, which stops and lets you analyze Tensors
  # after each Session.run
  tf_debug: False
  # Disables logging and saving checkpoints
  no_log: False
  # Displays debugging images with results every N steps. Debug mode must be
  # enabled
  display_every_steps: 5000
  # Display debugging images every N seconds.
  display_every_secs:
  # Shuffle the dataset. It should only be disabled when trying to reproduce
  # some problem on some sample
  random_shuffle: False
  # Save Tensorboard timeline
  save_timeline: False
  # The frequency, in seconds, that a checkpoint is saved.
  save_checkpoint_secs: 600
  # The frequency, in number of global steps, that the summaries are written to disk
  save_summaries_steps:
  # The frequency, in secs, that the summaries are written to disk.
  # If both save_summaries_steps and save_summaries_secs are set to empty, then the
  # default summary saver isn't used
  save_summaries_secs: 30
  # Run TensorFlow using full_trace mode for memory and running time logging
  # Debug mode must be enabled.
  full_trace: False
  # Clip gradients by norm, making sure the maximum value is 10.
  clip_by_norm: False
  # Learning rate config.
  learning_rate:
    # Because we're using kwargs, we want the learning_rate dict to be replaced
    # as a whole.
    _replace: True
    # Learning rate decay method ((empty), "none", piecewise_constant, exponential_decay, polynomial_decay)
    # You can define different decay methods using `decay_method` and defining all the necessary arguments.
    decay_method:
    # Oficial initial learning rate is 0.001 (with batch size 32)
    learning_rate: 0.0003

  # Optimizer config
  optimizer:
    # Because we're using kwargs, we want the optimizer dict to be replaced
    # as a whole.
    _replace: True
    # Type of optimizer to use (momentum, adam, gradient_descent, rmsprop)
    type: momentum
    # Oficial momentum is 0.9
    momentum: 0.5

  # Number of epochs (complete dataset batches) to run
  num_epochs: 10000

  # Image visualization mode, options = train, eval, debug, (empty). Default=(empty)
  image_vis: debug

eval:
  # Image visualization mode, options = train, eval, debug, (empty). Default=(empty)
  image_vis: eval

dataset:
  type: tfrecord
  # From which directory to read the dataset
  dir: datasets/voc/tf
  # Which split of tfrecords to look for
  split: train
  image_preprocessing:
    # Resize the input image to fixed_height and fixed_width
    fixed_height: 300
    fixed_width: 300

  # Data augmentation techniques
  data_augmentation:
    - flip:
        left_right: True
        up_down: False
        prob: 0.5
    - patch:
        min_height: 30
        min_width: 30
        prob: 0.5
    - distortion:
        brightness:
          max_delta: 0.2
        hue:
          max_delta: 0.2
        saturation:
          lower: 0.5
          upper: 1.5
        prob: 0.5
   - expand:
       prob: 0.5

model:
  type: ssd
  network:
    # Total number of classes to predict
    num_classes: 20

  base_network:
    # Which type of pretrained network to use
    architecture: truncated_vgg_16
    # Should we train the pretrained network
    trainable: True
    # From which file to load the weights
    weights:
    # Should we download weights if not available
    download: True
    arg_scope:
      # The l2 regularization coefficient.
      weight_decay: 0.0005
    dropout_keep_prob: 1.0

  loss:
    # Loss weights for calculating the total loss
    localization_loss_weight: 1.0

  anchors:
    # Number of anchors per point in each feature map. This anchors must be in order
    # and they must match with the number of total endpoints
    anchors_per_point: [4, 6, 6, 6, 4, 4]
    # Aspect ratios used for generating anchors. If there is more ratios than
    # anchors per point, it will be used only the firsts to calculate the sclaes.
    # By now, the ratio of 1 must always be at the begin of the array
    ratios: [1, 0.5, 2, 0.333, 3]
    # Minimum and maximum scale of anchors
    min_scale: 0.10
    max_scale: 0.88

  target:
    # The ratio of positives-negatives will be 1:`hard_negative_ratio`
    hard_negative_ratio: 3.
    # Threshold with GT to be considered positive
    foreground_threshold: 0.5
    # High and low threshold with GT to be considered negative
    background_threshold_high: 0.2
    background_threshold_low: 0.0  # Not used at the moment

  proposals:
    # Maximum total detections for an image (sorted by score)
    total_max_detections: 100
    # Maximum number of detections for each class
    # Not used in paper, so I set class_max_detections = total_max_detections
    class_max_detections: 100
    # NMS threshold used to remove "almost duplicate" of the same class
    class_nms_threshold: 0.45
    # Minimum prob to be used as proposed object
    # Setting it to 0.01 is for precision-recall scoring purposes though,
    # in order to get some usable final predictions, the confidence
    # threshold must be set higher, e.g. to 0.5.
    # ( Acording to: https://github.com/pierluigiferrari/ssd_keras/blob/d1b4e7e218af7d31314b9927c08e735e54f5af30/ssd300_training.ipynb#L548 )
    min_prob_threshold: 0.00001
    # Filter proposals from anchors partially outside the image.
    filter_outside_anchors: True

  # Variance, although it should probably be called 'box_scale_factors' as discussed here: https://github.com/rykov8/ssd_keras/issues/53
  # Used to scale location prediction and confidence prediction
  # Also discussed here: https://github.com/weiliu89/caffe/issues/155
  variances: [0.1, 0.2]
